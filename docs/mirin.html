<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.4" />
<title>mirin API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>mirin</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import srt 
import os 
import zipfile
import pysrt 
import sys
import re
import json
import operator
import click
import genanki 
import time

from dashi.search import Kanji
from dashi.creator import add_card, DECK_NO, MODEL
DATABASE_PATH = &#39;./databases&#39;


def make_database(subtitle_array): 
    &#34;&#34;&#34;
    Args:
        subtitle array: an array of subtitles(strings)
    
    
    Returns:
        sorted dictionary database of kanji
        Goes through each if the subtitles&#39; strings character by character, checks if they are kanji, and
        if they are, adds them to the database.
        I can see this being really slow considering it&#39;s a double for loop
    &#34;&#34;&#34;
    database = {}
    # This is subtitle level
    for sub in subtitle_array: 
        # This is the sentence level
        for char in sub.text:
            if Kanji.is_kanji(char):
                if not database.get(char):
                    # Initialize database entry and zero count 
                    database[char] = 0
                database[char]+=1
                # count +=1 
    return {k: v for k, v in sorted(database.items(), key=lambda item: item[1])}

def handle_srt(): 
    &#34;&#34;&#34;
    Args: 
        None
    Returns:
        None
        This does most of the legwork. 
        Goes through the /extracted/ directory, and finds the media&#39;s subfolder.
        Then it goes through each of the individual subtitle file&#39;s contents. 
        It calls upon the make_database file and uses it to create databases for each &#34;episode&#34;
    &#34;&#34;&#34;
    for subtitle_directory in os.listdir(&#39;./extracted/&#39;):
        print(subtitle_directory)
        count = 0 
        for subtitle in os.listdir(&#39;./extracted/{}&#39;.format(subtitle_directory)):
            subs = pysrt.open(&#39;./extracted/{0}/{1}&#39;.format(subtitle_directory, subtitle), encoding=&#39;utf-8-sig&#39;)

            sorted_database = make_database(subs)
            current_db_path = &#34;{0}/{1}/&#34;.format(DATABASE_PATH, subtitle_directory)
            if not os.path.isdir(current_db_path):
                os.mkdir(DATABASE_PATH)
                os.mkdir(current_db_path)
            with open(current_db_path + &#34;{}.json&#34;.format(subtitle), &#39;w+&#39;, encoding=&#39;utf8&#39;) as f: 
                json.dump(sorted_database, f, ensure_ascii=False)
            count += 1


def extract_subs(extract): 
    
    &#34;&#34;&#34;
    Args: 
        None
    Returns: 
        None
        Goes through the root directory, and does some logic to figure out whether or not something is subtitle archive.
        If it is, it extracts it to its own subdirectory within the /extracted/ directory.
    &#34;&#34;&#34;
    for file in os.listdir(): 
        if not os.path.isfile(file): 
            continue 

        fn = file.split(&#39;.&#39;)
        if not fn[1] in [&#39;zip&#39;, &#39;rar&#39;]:
            continue 
    
        # Otherwise it&#39;s a zipfile! 
        with zipfile.ZipFile(&#39;./{0}&#39;.format(file), &#39;r&#39;) as subtitle_archive: 
            subtitle_archive.extractall(&#34;./extracted/{}&#34;.format(fn[0]))

def add_card_helper(data, deck, kanji): 
    meanings = &#34;&#34;
    on_readings = &#34;&#34;
    kun_readings = &#34;&#34;
    meanings = &#39;, &#39;.join(data[&#34;meanings&#34;])
    on_readings = &#39;, &#39;.join(data[&#34;on_readings&#34;])
    kun_readings = &#39;, &#39;.join(data[&#34;kun_readings&#34;])
    
    time.sleep(2) # Waiting 2 seconds is probably fine and permissable
    base = &#34;&#34;&#34;
    on reading(s): {0}
    kun readings: {1}
    meaning(s): {2}
    &#34;&#34;&#34;.format(on_readings, kun_readings, meanings)
    
    my_note = genanki.Note(
        model=MODEL,
        fields=[kanji, base])


    deck.add_note(my_note)

@click.group(invoke_without_command=True)
@click.option(&#39;--path&#39;, required=True, type=click.Path(exists=True), help=&#39;Path to the database for the desired media in this format: ./databases/media/&#39;)
@click.option(&#39;--threshold&#39;, type=int, default=100, show_default=True, help=&#39;Lower bound of usage threshold for a kanji to be included in the SRS deck.&#39;)
@click.option(&#39;--extract&#39;, type=bool, default=False, show_default=True, help=&#39;If True, all zip files in the root directory will be extracted into the /extracted/ directory. Set it to True for the first time.&#39;)
@click.option(&#39;--jlpt&#39;, type=str, default=None, help=&#39;Only add kanji which are part of this JLPT level or lower. Case insensitive. I.e: N5, N4, N3...&#39;)
def mirin(path, threshold, extract, jlpt): 
    &#34;&#34;&#34; 
    this function is the main cli call. 
    Args:
        path: database path
        threshold: lower bound on the usage thresholdfor a kanji to be included. 
        extract: boolean flag 
        jlpt: string denoting the JLPT limit (inclusive)
    Click command arguments: 
    Options:
        --path PATH          Path to the database for the desired media in this
                            format: ./databases/media/  [required]

        --threshold INTEGER  Lower bound of usage threshold for a kanji to be
                            included in the SRS deck.  [default: 100]

        --extract BOOLEAN    If True, all zip files in the root directory will be
                            extracted into the /extracted/ directory. Set it to
                            True for the first time.  [default: False]

        --jlpt TEXT          Only add kanji which are part of this JLPT level or
                            lower. Case insensitive. I.e: N5, N4, N3...

        --help               Show this message and exit.
    &#34;&#34;&#34;
    if jlpt is not None:
        
        if not re.search(&#34;^n[1-5]$&#34;, jlpt.lower()): 
            raise SystemExit(&#34;Improper input for --jlpt flag: {}&#34;.format(jlpt))
        jlpt = int(jlpt[1]) # grab the integer level
    if extract: 
        extract_subs()
    
    for filename in os.listdir(path):
        # Individual deck level
        deck = genanki.Deck(DECK_NO, &#39;&#39;)
        print(filename)
        
        print(jlpt)
        with open(path+filename, &#39;r&#39;, encoding=&#39;utf-8&#39;) as file: 
            count = 0
            kanji_database = json.load(file)
            for kanji, frequency in kanji_database.items():
                if (jlpt is not None) and frequency &gt;= threshold: 
                    
                    r = Kanji.search_kanji(kanji)
                    if int(r.get(&#39;jlpt&#39;)) &lt;= jlpt:
                        print(&#34;jlpt level passes&#34;)
                    # So in this case, the JLPT flag isn&#39;t None, and it is above the threshold and it&#39;s below the upper bound of JLPT.
                        add_card_helper(r, deck, kanji)
                        continue
                    else: 
                        print(&#34;jlpt level doesnt pass&#34;)
                        continue

                    # in this case the JLPT level is None, so just add as normal since it&#39;s above the treshold.
                elif frequency &gt;= threshold:
                    r = Kanji.search_kanji(kanji)
                    add_card_helper(r, deck, kanji)
                else: 
                    # Doesn&#39;t qualify by any criteria
                     continue 
        count +=1       
        if not os.path.isdir(&#39;./decks/&#39;):
            os.mkdir(&#39;./decks/&#39;)
        genanki.Package(deck).write_to_file(&#34;./decks/{0}_Deck{1}.apkg&#34;.format(filename, count))

    
if __name__ == &#34;__main__&#34;:
    mirin()
    extract_subs()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="mirin.add_card_helper"><code class="name flex">
<span>def <span class="ident">add_card_helper</span></span>(<span>data, deck, kanji)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_card_helper(data, deck, kanji): 
    meanings = &#34;&#34;
    on_readings = &#34;&#34;
    kun_readings = &#34;&#34;
    meanings = &#39;, &#39;.join(data[&#34;meanings&#34;])
    on_readings = &#39;, &#39;.join(data[&#34;on_readings&#34;])
    kun_readings = &#39;, &#39;.join(data[&#34;kun_readings&#34;])
    
    time.sleep(2) # Waiting 2 seconds is probably fine and permissable
    base = &#34;&#34;&#34;
    on reading(s): {0}
    kun readings: {1}
    meaning(s): {2}
    &#34;&#34;&#34;.format(on_readings, kun_readings, meanings)
    
    my_note = genanki.Note(
        model=MODEL,
        fields=[kanji, base])


    deck.add_note(my_note)</code></pre>
</details>
</dd>
<dt id="mirin.extract_subs"><code class="name flex">
<span>def <span class="ident">extract_subs</span></span>(<span>extract)</span>
</code></dt>
<dd>
<div class="desc"><p>Args:
None
Returns:
None
Goes through the root directory, and does some logic to figure out whether or not something is subtitle archive.
If it is, it extracts it to its own subdirectory within the /extracted/ directory.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_subs(extract): 
    
    &#34;&#34;&#34;
    Args: 
        None
    Returns: 
        None
        Goes through the root directory, and does some logic to figure out whether or not something is subtitle archive.
        If it is, it extracts it to its own subdirectory within the /extracted/ directory.
    &#34;&#34;&#34;
    for file in os.listdir(): 
        if not os.path.isfile(file): 
            continue 

        fn = file.split(&#39;.&#39;)
        if not fn[1] in [&#39;zip&#39;, &#39;rar&#39;]:
            continue 
    
        # Otherwise it&#39;s a zipfile! 
        with zipfile.ZipFile(&#39;./{0}&#39;.format(file), &#39;r&#39;) as subtitle_archive: 
            subtitle_archive.extractall(&#34;./extracted/{}&#34;.format(fn[0]))</code></pre>
</details>
</dd>
<dt id="mirin.handle_srt"><code class="name flex">
<span>def <span class="ident">handle_srt</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Args:
None</p>
<h2 id="returns">Returns</h2>
<p>None
This does most of the legwork.
Goes through the /extracted/ directory, and finds the media's subfolder.
Then it goes through each of the individual subtitle file's contents.
It calls upon the make_database file and uses it to create databases for each "episode"</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def handle_srt(): 
    &#34;&#34;&#34;
    Args: 
        None
    Returns:
        None
        This does most of the legwork. 
        Goes through the /extracted/ directory, and finds the media&#39;s subfolder.
        Then it goes through each of the individual subtitle file&#39;s contents. 
        It calls upon the make_database file and uses it to create databases for each &#34;episode&#34;
    &#34;&#34;&#34;
    for subtitle_directory in os.listdir(&#39;./extracted/&#39;):
        print(subtitle_directory)
        count = 0 
        for subtitle in os.listdir(&#39;./extracted/{}&#39;.format(subtitle_directory)):
            subs = pysrt.open(&#39;./extracted/{0}/{1}&#39;.format(subtitle_directory, subtitle), encoding=&#39;utf-8-sig&#39;)

            sorted_database = make_database(subs)
            current_db_path = &#34;{0}/{1}/&#34;.format(DATABASE_PATH, subtitle_directory)
            if not os.path.isdir(current_db_path):
                os.mkdir(DATABASE_PATH)
                os.mkdir(current_db_path)
            with open(current_db_path + &#34;{}.json&#34;.format(subtitle), &#39;w+&#39;, encoding=&#39;utf8&#39;) as f: 
                json.dump(sorted_database, f, ensure_ascii=False)
            count += 1</code></pre>
</details>
</dd>
<dt id="mirin.make_database"><code class="name flex">
<span>def <span class="ident">make_database</span></span>(<span>subtitle_array)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<p>subtitle array: an array of subtitles(strings)</p>
<h2 id="returns">Returns</h2>
<p>sorted dictionary database of kanji
Goes through each if the subtitles' strings character by character, checks if they are kanji, and
if they are, adds them to the database.
I can see this being really slow considering it's a double for loop</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_database(subtitle_array): 
    &#34;&#34;&#34;
    Args:
        subtitle array: an array of subtitles(strings)
    
    
    Returns:
        sorted dictionary database of kanji
        Goes through each if the subtitles&#39; strings character by character, checks if they are kanji, and
        if they are, adds them to the database.
        I can see this being really slow considering it&#39;s a double for loop
    &#34;&#34;&#34;
    database = {}
    # This is subtitle level
    for sub in subtitle_array: 
        # This is the sentence level
        for char in sub.text:
            if Kanji.is_kanji(char):
                if not database.get(char):
                    # Initialize database entry and zero count 
                    database[char] = 0
                database[char]+=1
                # count +=1 
    return {k: v for k, v in sorted(database.items(), key=lambda item: item[1])}</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="mirin.add_card_helper" href="#mirin.add_card_helper">add_card_helper</a></code></li>
<li><code><a title="mirin.extract_subs" href="#mirin.extract_subs">extract_subs</a></code></li>
<li><code><a title="mirin.handle_srt" href="#mirin.handle_srt">handle_srt</a></code></li>
<li><code><a title="mirin.make_database" href="#mirin.make_database">make_database</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.4</a>.</p>
</footer>
</body>
</html>